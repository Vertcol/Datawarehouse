{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported sales tables\n",
      "Imported staff tables\n",
      "Imported crm tables\n",
      "Imported inventory table\n",
      "Imported sales product forecast table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pyodbc\n",
    "\n",
    "select_tables = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "\n",
    "sales_con = sqlite3.connect(\"go_sales.sqlite\")\n",
    "sales_tables = pd.read_sql_query(select_tables, sales_con)\n",
    "\n",
    "sales_country       = pd.read_sql_query(\"SELECT * FROM country;\", sales_con)\n",
    "order_details       = pd.read_sql_query(\"SELECT * FROM order_details;\", sales_con)\n",
    "order_header        = pd.read_sql_query(\"SELECT * FROM order_header;\", sales_con)\n",
    "order_method        = pd.read_sql_query(\"SELECT * FROM order_method;\", sales_con)\n",
    "product             = pd.read_sql_query(\"SELECT * FROM product;\", sales_con)\n",
    "product_line        = pd.read_sql_query(\"SELECT * FROM product_line;\", sales_con)\n",
    "product_type        = pd.read_sql_query(\"SELECT * FROM product_type;\", sales_con)\n",
    "sales_retailer_site = pd.read_sql_query(\"SELECT * FROM retailer_site;\", sales_con)\n",
    "return_reason       = pd.read_sql_query(\"SELECT * FROM return_reason;\", sales_con)\n",
    "returned_item       = pd.read_sql_query(\"SELECT * FROM returned_item;\", sales_con)\n",
    "sales_branch        = pd.read_sql_query(\"SELECT * FROM sales_branch;\", sales_con)\n",
    "sales_staff         = pd.read_sql_query(\"SELECT * FROM sales_staff;\", sales_con)\n",
    "SALES_TARGETData    = pd.read_sql_query(\"SELECT * FROM SALES_TARGETData;\", sales_con)\n",
    "sqlite_sequence     = pd.read_sql_query(\"SELECT * FROM sqlite_sequence;\", sales_con)\n",
    "print(\"Imported sales tables\")\n",
    "\n",
    "staff_con = sqlite3.connect(\"go_staff.sqlite\")\n",
    "staff_tables = pd.read_sql_query(select_tables, staff_con)\n",
    "\n",
    "course            = pd.read_sql_query(\"SELECT * FROM course;\", staff_con)\n",
    "sales_branch      = pd.read_sql_query(\"SELECT * FROM sales_branch;\", staff_con)\n",
    "sales_staff       = pd.read_sql_query(\"SELECT * FROM sales_staff;\", staff_con)\n",
    "satisfaction      = pd.read_sql_query(\"SELECT * FROM satisfaction;\", staff_con)\n",
    "satisfaction_type = pd.read_sql_query(\"SELECT * FROM satisfaction_type;\", staff_con)\n",
    "training          = pd.read_sql_query(\"SELECT * FROM training;\", staff_con)\n",
    "print(\"Imported staff tables\")\n",
    "\n",
    "crm_con = sqlite3.connect(\"go_crm.sqlite\")\n",
    "crm_tables = pd.read_sql_query(select_tables, crm_con)\n",
    "                           \n",
    "age_group             = pd.read_sql_query(\"SELECT * FROM age_group;\", crm_con)\n",
    "crm_country           = pd.read_sql_query(\"SELECT * FROM country;\", crm_con)\n",
    "retailer              = pd.read_sql_query(\"SELECT * FROM retailer;\", crm_con)\n",
    "retailer_contact      = pd.read_sql_query(\"SELECT * FROM retailer_contact;\", crm_con)\n",
    "retailer_headquarters = pd.read_sql_query(\"SELECT * FROM retailer_headquarters;\", crm_con)\n",
    "retailer_segment      = pd.read_sql_query(\"SELECT * FROM retailer_segment;\", crm_con)\n",
    "crm_retailer_site     = pd.read_sql_query(\"SELECT * FROM retailer_site;\", crm_con)\n",
    "retailer_type         = pd.read_sql_query(\"SELECT * FROM retailer_type;\", crm_con)\n",
    "sales_demographic     = pd.read_sql_query(\"SELECT * FROM sales_demographic;\", crm_con)\n",
    "sales_territory       = pd.read_sql_query(\"SELECT * FROM sales_territory;\", crm_con)\n",
    "print(\"Imported crm tables\")\n",
    "\n",
    "inventory_level = pd.read_csv(\"GO_SALES_INVENTORY_LEVELSData.csv\")\n",
    "print(\"Imported inventory table\")\n",
    "\n",
    "sales_forecast = pd.read_csv(\"GO_SALES_PRODUCT_FORECASTData.csv\")\n",
    "print(\"Imported sales product forecast table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Server connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "servername = 'DESKTOP-9F8A8PF\\\\MSSQLSERVER01'\n",
    "database = 'Datawarehouse'\n",
    "\n",
    "sql_server_conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={servername};DATABASE={database};Trusted_Connection=yes\")\n",
    "cursor = sql_server_conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Method to merge two tables flexibly\n",
    "- NaN values of one dataframe can be filled by the other dataframe\n",
    "- Uses all available columns\n",
    "- Errors when a row of the two dataframes doesn't match (df1 has 'A' and df2 has 'B' in row)\n",
    "\"\"\"\n",
    "def merge_tables(df1, df2, index_col):\n",
    "    # Ensure 'CODE' is set as the index for both DataFrames\n",
    "    if index_col not in df1.columns or index_col not in df2.columns:\n",
    "        raise KeyError(f\"{index_col} must be a column in both DataFrames.\")\n",
    "    \n",
    "    df1 = df1.set_index(index_col)\n",
    "    df2 = df2.set_index(index_col)\n",
    "\n",
    "    # Identify common and exclusive columns\n",
    "    common_columns = df1.columns.intersection(df2.columns)\n",
    "    exclusive_df1 = df1.columns.difference(df2.columns)\n",
    "    exclusive_df2 = df2.columns.difference(df1.columns)\n",
    "\n",
    "    # Concatenate exclusive columns from each DataFrame onto the other\n",
    "    df1_combined = pd.concat([df1, df2[exclusive_df2]], axis=1, sort=False)\n",
    "    df2_combined = pd.concat([df2, df1[exclusive_df1]], axis=1, sort=False)\n",
    "\n",
    "    # Resolve common columns with nulls and conflicts\n",
    "    for col in common_columns:\n",
    "        # Align the Series from both DataFrames for comparison\n",
    "        series1, series2 = df1_combined[col].align(df2_combined[col])\n",
    "\n",
    "        # Check for conflicts (non-null values that do not match)\n",
    "        conflict_mask = (~series1.isnull() & ~series2.isnull() & (series1 != series2))\n",
    "        if conflict_mask.any():\n",
    "            raise ValueError(f\"Merge failed due to conflict in column '{col}'\")\n",
    "\n",
    "        # Use values from df2 where df1 is null (prioritizing df1 values)\n",
    "        df1_combined[col] = series1.combine_first(series2)\n",
    "\n",
    "    return df1_combined\n",
    "\n",
    "# Merge duplicate tables into single table\n",
    "retailer_site = merge_tables(sales_retailer_site, crm_retailer_site, 'RETAILER_SITE_CODE')\n",
    "# Column name mismatch\n",
    "sales_country = sales_country.rename(columns={'COUNTRY': 'COUNTRY_EN'})\n",
    "country = merge_tables(sales_country, crm_country, 'COUNTRY_CODE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dicionary to rename all original columns to their Data Warehouse equivalent\n",
    "-  Types are encoded in the column name (COLUMN_NAME_type)\n",
    "\"\"\"\n",
    "rename_mapping = {\n",
    "    'ACTIVE_INDICATOR': 'ACTIVE_INDICATOR_bool',\n",
    "    'ADDRESS1': 'ADDRESS1_address',\n",
    "    'ADDRESS2': 'ADDRESS2_address',\n",
    "    'CITY': 'CITY_name',\n",
    "    'COMPANY_NAME': 'COMPANY_name',\n",
    "    'COUNTRY_CODE': 'COUNTRY_id',\n",
    "    'COUNTRY_EN': 'COUNTRY_name',\n",
    "    'COURSE_CODE': 'COURSE_id',\n",
    "    'COURSE_DESCRIPTION': 'COURSE_description',\n",
    "    'COUNTRY_LANGUAGE_code': 'COUNTRY_LANGUAGE_code',\n",
    "    'CURRENCY_NAME': 'CURRENCY_name',\n",
    "    'DATE_HIRED': 'DATE_HIRED_date',\n",
    "    'DESCRIPTION': 'PRODUCT_description',\n",
    "    'EMAIL': 'EMAIL_address',\n",
    "    'EXPECTED_VOLUME': 'EXPECTED_VOLUME_number',\n",
    "    'EXTENSION': 'EXTENSION_number',\n",
    "    'E_MAIL': 'EMAIL_address',\n",
    "    'FAX': 'FAX_phone',\n",
    "    'FIRST_NAME': 'FIRST_NAME_name',\n",
    "    'FLAG_IMAGE': 'FLAG_image',\n",
    "    'FULL_NAME': 'FULL_NAME_name',\n",
    "    'GENDER': 'GENDER_char',\n",
    "    'INTRODUCTION_DATE': 'PRODUCT_INTRODUCTION_DATE_date',\n",
    "    'INVENTORY_COUNT': 'INVENTORY_COUNT_number',\n",
    "    'INVENTORY_MONTH': 'INVENTORY_MONTH_number',\n",
    "    'INVENTORY_YEAR': 'INVENTORY_YEAR_number',\n",
    "    'JOB_POSITION_EN': 'JOB_POSITION_name',\n",
    "    'LANGUAGE': 'LANGUAGE_name',\n",
    "    'LAST_NAME': 'LAST_NAME_name',\n",
    "    'MANAGER_CODE': 'MANAGER_id',\n",
    "    'MARGIN': 'PRODUCT_MARGIN_percentage',\n",
    "    'MONTH': 'MONTH_number',\n",
    "    'ORDER_DATE': 'ORDER_DATE_date',\n",
    "    'ORDER_DETAIL_CODE': 'ORDER_DETAIL_id',\n",
    "    'ORDER_METHOD_CODE': 'ORDER_METHOD_id',\n",
    "    'ORDER_METHOD_EN': 'ORDER_METHOD_name',\n",
    "    'ORDER_NUMBER': 'ORDER_TABLE_id',\n",
    "    'PHONE': 'PHONE_phone',\n",
    "    'POSITION_EN': 'POSITION_name',\n",
    "    'POSTAL_ZONE': 'POSTAL_ZONE_code',\n",
    "    'PRODUCTION_COST': 'PRODUCT_PRODUCTION_COST_money',\n",
    "    'PRODUCT_IMAGE': 'PRODUCT_image',\n",
    "    'PRODUCT_LINE_CODE': 'PRODUCT_LINE_id',\n",
    "    'PRODUCT_LINE_EN': 'PRODUCT_LINE_name',\n",
    "    'PRODUCT_NAME': 'PRODUCT_name',\n",
    "    'PRODUCT_NUMBER': 'PRODUCT_id',\n",
    "    'QUANTITY': 'QUANTITY_number',\n",
    "    'REGION': 'REGION_name',\n",
    "    'RETAILER_CODE': 'RETAILER_id',\n",
    "    'RETAILER_CODEMR': 'RETAILER_MR_id',\n",
    "    'RETAILER_CONTACT_CODE': 'RETAILER_CONTACT_id',\n",
    "    'RETAILER_NAME': 'RETAILER_name',\n",
    "    'RETAILER_SITE_CODE': 'RETAILER_SITE_id',\n",
    "    'RETAILER_TYPE_CODE': 'RETAILER_TYPE_id',\n",
    "    'RETAILER_TYPE_EN': 'RETAILER_TYPE_name',\n",
    "    'RETURN_CODE': 'RETURNS_id',\n",
    "    'RETURN_DATE': 'RETURN_DATE_date',\n",
    "    'RETURN_DESCRIPTION_EN': 'RETURN_REASON_description',\n",
    "    'RETURN_QUANTITY': 'RETURN_QUANTITY_number',\n",
    "    'RETURN_REASON_CODE': 'RETURN_REASON_id',\n",
    "    'SALES_BRANCH_CODE': 'SALES_BRANCH_id',\n",
    "    'SALES_STAFF_CODE': 'SALES_STAFF_id',\n",
    "    'SALES_TERRITORY_CODE': 'SALES_TERRITORY_id',\n",
    "    'SATISFACTION_TYPE_CODE': 'SATISFACTION_TYPE_id',\n",
    "    'SATISFACTION_TYPE_DESCRIPTION': 'SATISFACTION_TYPE_description',\n",
    "    'SEGMENT_CODE': 'SEGMENT_code',\n",
    "    'SEGMENT_LANGUAGE_code': 'SEGMENT_LANGUAGE_code',\n",
    "    'TERRITORY_NAME_EN': 'TERRITORY_name',\n",
    "    'UNIT_COST': 'UNIT_COST_money',\n",
    "    'UNIT_PRICE': 'UNIT_PRICE_money',\n",
    "    'UNIT_SALE_PRICE': 'UNIT_SALE_PRICE_money',\n",
    "    'WORK_PHONE': 'WORK_PHONE_phone',\n",
    "    'YEAR': 'YEAR_number',\n",
    "    'TARGET_id': 'TARGET_id'\n",
    "}\n",
    "\n",
    "# List of all vetted columns\n",
    "valid_columns = set(rename_mapping.values())\n",
    "\n",
    "# Filters out all columns of dataframe that aren't typed\n",
    "def filterColumns(dataframe):\n",
    "    valid_columns_set = set(valid_columns)\n",
    "    actual_columns_set = set(dataframe.columns)\n",
    "    intersection_columns = list(actual_columns_set.intersection(valid_columns_set))\n",
    "\n",
    "    # Use the intersection result to filter columns from dataframe\n",
    "    return dataframe[intersection_columns]\n",
    "\n",
    "# Filters out all columns of dataframe that aren't typed\n",
    "def excludeColumns(dataframe, column_names):\n",
    "    return dataframe[dataframe.columns.difference(column_names)]\n",
    "\n",
    "def sizeCheck(dataframe, expected_column_count):\n",
    "    actual_column_count = len(dataframe.columns)\n",
    "    if actual_column_count == expected_column_count:\n",
    "        print(f'Table has {expected_column_count} columns')\n",
    "    else:\n",
    "        raise Exception(f'Table has {actual_column_count} columns, expected {expected_column_count}')\n",
    "\n",
    "\"\"\"\n",
    "Get the last slice of a string\n",
    "\"\"\"\n",
    "def getTypes():\n",
    "    types = {}\n",
    "    for column in rename_mapping.values():\n",
    "        column_type = column.rsplit('_', 1)[1]\n",
    "        types[column_type] = ''\n",
    "    return types\n",
    "\n",
    "\"\"\"\n",
    "Uses the column name to derive a SQL Server compatible type\n",
    "- The type is derived from the column name (COLUMN_NAME_type)\n",
    "- Column names without a type are invalid\n",
    "\"\"\"\n",
    "def columnType(column_name):\n",
    "    column_types = {\n",
    "        'name': 'NVARCHAR(80)',\n",
    "        'image': 'NVARCHAR(60)',\n",
    "        'id': 'INT',\n",
    "        'description': 'NTEXT',\n",
    "        'money': 'DECIMAL(19,4)',\n",
    "        'percentage': 'DECIMAL(12,12)',\n",
    "        'date': 'NVARCHAR(30)',\n",
    "        'code': 'NVARCHAR(40)',\n",
    "        'char': 'CHAR(1)',\n",
    "        'number': 'INT',\n",
    "        'phone': 'NVARCHAR(30)',\n",
    "        'address': 'NVARCHAR(80)',\n",
    "        'bool': 'BIT',\n",
    "    }\n",
    "\n",
    "    err = ''\n",
    "    try:\n",
    "        return column_types[column_name.rsplit('_', 1)[1]]\n",
    "    except IndexError:\n",
    "        err = \"Column name doesn't contain a type\"\n",
    "    except KeyError:\n",
    "        err = \"Column type not found\"\n",
    "    raise Exception(err)\n",
    "\n",
    "\"\"\"\n",
    "Method to insert dataframe data into SQL server\n",
    "\"\"\"\n",
    "def createTable(tablename, dataframe, PK):\n",
    "    SK = ''\n",
    "    if PK == None:\n",
    "        SK = f'SK_{tablename}'\n",
    "        columns = ''\n",
    "    else:\n",
    "        SK = f'SK_{PK}'\n",
    "        columns = f'{PK} {columnType(PK)} NOT NULL'\n",
    "    # Add Primary Key as third column\n",
    "    \n",
    "    # Add all the other columns\n",
    "    for column in dataframe.columns:\n",
    "        if column != PK: # PK is already added\n",
    "            columns += f', {column} {columnType(column)}'\n",
    "\n",
    "    surogate_columns = f\"{SK} INT IDENTITY(1,1) NOT NULL PRIMARY KEY, Timestamp DATETIME NOT NULL DEFAULT(GETDATE())\"\n",
    "\n",
    "    # Create the command\n",
    "    command = f\"CREATE TABLE {tablename} ({surogate_columns}, {columns})\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        cursor.commit()\n",
    "    except pyodbc.Error as e:\n",
    "        if 'There is already an object named' in str(e):\n",
    "            print('Table already exists in database')\n",
    "        else:\n",
    "            raise(e)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Method to insert dataframe data into SQL server\n",
    "\"\"\"\n",
    "def insertTable(tablename, dataframe, PK):\n",
    "    # Add Primary Key as first column\n",
    "    columns = PK\n",
    "    \n",
    "    # Add all the other columns\n",
    "    for column in dataframe.columns:\n",
    "        if column != PK: # PK is already added\n",
    "            columns += f', {column}'\n",
    "    \n",
    "    # Execute inserts\n",
    "    for i, row in dataframe.iterrows():\n",
    "        values = ''\n",
    "        values += str(row[PK])\n",
    "\n",
    "        for column in dataframe.columns:\n",
    "            if column != PK: # PK is already added\n",
    "                try:\n",
    "                    val = str(row[column]).replace(\"'\",\"''\")\n",
    "                    if val != 'None':\n",
    "                        values += f\", '{val}'\"\n",
    "                    else:\n",
    "                        values += f\", NULL\"\n",
    "                except AttributeError:\n",
    "                    values += f\", NULL\"\n",
    "\n",
    "        command = f\"INSERT INTO {tablename} ({columns}) VALUES ({values});\\n\"\n",
    "        \n",
    "        cursor.execute(command)\n",
    "    \n",
    "    try:\n",
    "        cursor.commit()\n",
    "    except pyodbc.Error as e:\n",
    "        if 'There is already an object named' in str(e):\n",
    "            print('Table already exists in database')\n",
    "        else:\n",
    "            print(command)\n",
    "            print(e)\n",
    "\n",
    "# Tables to create at end         \n",
    "etl_tables = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 10 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "product_etl = pd.merge(product, product_type, on=\"PRODUCT_TYPE_CODE\")\n",
    "product_etl = pd.merge(product_etl, product_line, on=\"PRODUCT_LINE_CODE\")\n",
    "\n",
    "# Rename\n",
    "product_etl = product_etl.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "product_etl = filterColumns(product_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(product_etl,10)\n",
    "product_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Product', product_etl, 'PRODUCT_id'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Staff ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 24 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "sales_staff_etl = pd.merge(sales_staff, sales_branch, on='SALES_BRANCH_CODE')\n",
    "sales_staff_etl = pd.merge(sales_staff_etl, country, on='COUNTRY_CODE')\n",
    "sales_staff_etl = pd.merge(sales_staff_etl, sales_territory, on='SALES_TERRITORY_CODE')\n",
    "\n",
    "# Add\n",
    "sales_staff_etl['FULL_NAME'] = sales_staff_etl['FIRST_NAME'] + ' ' + sales_staff_etl['LAST_NAME']\n",
    "\n",
    "\n",
    "# Rename\n",
    "sales_staff_etl = sales_staff_etl.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "sales_staff_etl = filterColumns(sales_staff_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(sales_staff_etl,24)\n",
    "sales_staff_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Sales_Staff', sales_staff_etl, 'SALES_STAFF_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satisfaction type ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Rename\n",
    "satisfaction_type_etl = satisfaction_type.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "satisfaction_type_etl = filterColumns(satisfaction_type_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(satisfaction_type_etl,2)\n",
    "satisfaction_type_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Satisfaction_Type', satisfaction_type_etl, 'SATISFACTION_TYPE_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Rename\n",
    "course_etl = course.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "course_etl = filterColumns(course_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(course_etl,2)\n",
    "course_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Course', course_etl, 'COURSE_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Forecast ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 4 columns\n"
     ]
    }
   ],
   "source": [
    "# Rename\n",
    "sales_forecast_etl = sales_forecast.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "sales_forecast_etl = filterColumns(sales_forecast_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(sales_forecast_etl,4)\n",
    "sales_forecast_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Sales_Forecast', sales_forecast_etl, 'PRODUCT_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventory Level ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 4 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INVENTORY_YEAR</th>\n",
       "      <th>INVENTORY_MONTH</th>\n",
       "      <th>PRODUCT_NUMBER</th>\n",
       "      <th>INVENTORY_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>1932</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>1400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>21705</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>9710</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>5616</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>12</td>\n",
       "      <td>111</td>\n",
       "      <td>1128</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>12</td>\n",
       "      <td>112</td>\n",
       "      <td>7810</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>12</td>\n",
       "      <td>113</td>\n",
       "      <td>3485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>12</td>\n",
       "      <td>114</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>12</td>\n",
       "      <td>115</td>\n",
       "      <td>3310</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3888 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      INVENTORY_YEAR  INVENTORY_MONTH  PRODUCT_NUMBER  INVENTORY_COUNT\n",
       "2021               4               48            1932              NaN\n",
       "2021               4               49            1400              NaN\n",
       "2021               4               50           21705              NaN\n",
       "2021               4               51            9710              NaN\n",
       "2021               4               52            5616              NaN\n",
       "...              ...              ...             ...              ...\n",
       "2022              12              111            1128              NaN\n",
       "2022              12              112            7810              NaN\n",
       "2022              12              113            3485              NaN\n",
       "2022              12              114             350              NaN\n",
       "2022              12              115            3310              NaN\n",
       "\n",
       "[3888 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inventory_level_etl = inventory_level.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "inventory_level_etl = filterColumns(inventory_level_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(inventory_level_etl,4)\n",
    "inventory_level_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Inventory_Level', inventory_level_etl, 'PRODUCT_id'))\n",
    "inventory_level_etl\n",
    "inventory_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retailer Contact ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 24 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['EXTENSION_number', 'LANGUAGE_name', 'EMAIL_address', 'CITY_name',\n",
       "       'COUNTRY_name', 'JOB_POSITION_name', 'RETAILER_id', 'CURRENCY_name',\n",
       "       'ADDRESS1_address', 'FULL_NAME_name', 'ADDRESS2_address',\n",
       "       'ACTIVE_INDICATOR_bool', 'LAST_NAME_name', 'RETAILER_SITE_id',\n",
       "       'GENDER_char', 'SALES_TERRITORY_id', 'COUNTRY_id', 'FIRST_NAME_name',\n",
       "       'POSTAL_ZONE_code', 'FLAG_image', 'TERRITORY_name', 'FAX_phone',\n",
       "       'REGION_name', 'RETAILER_CONTACT_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge\n",
    "retailer_contact_etl = pd.merge(retailer_contact, retailer_site, on='RETAILER_SITE_CODE')\n",
    "retailer_contact_etl = pd.merge(retailer_contact_etl, country, on='COUNTRY_CODE')\n",
    "retailer_contact_etl = pd.merge(retailer_contact_etl, sales_territory, on='SALES_TERRITORY_CODE')\n",
    "\n",
    "# Add\n",
    "retailer_contact_etl['FULL_NAME'] = retailer_contact_etl['FIRST_NAME'] + ' ' + retailer_contact_etl['LAST_NAME']\n",
    "\n",
    "# Rename \n",
    "retailer_contact_etl = retailer_contact_etl.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "retailer_contact_etl = filterColumns(retailer_contact_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(retailer_contact_etl,24)\n",
    "retailer_contact_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Retailer_Contact', retailer_contact_etl, 'RETAILER_CONTACT_id'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retailer ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 22 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "retailer_etl = pd.merge(retailer, retailer_headquarters, on='RETAILER_CODEMR')\n",
    "retailer_etl = pd.merge(retailer_etl, retailer_type, on='RETAILER_TYPE_CODE')\n",
    "\n",
    "# Merge and rename language columns for clarity\n",
    "retailer_etl = pd.merge(retailer_etl, retailer_segment, on='SEGMENT_CODE').rename(columns={'LANGUAGE':'SEGMENT_LANGUAGE_code'})\n",
    "retailer_etl = pd.merge(retailer_etl, country, on='COUNTRY_CODE').rename(columns={'LANGUAGE':'COUNTRY_LANGUAGE_code'})\n",
    "\n",
    "# Exclude columns early due to merge naming conflicts\n",
    "retailer_etl = excludeColumns(retailer_etl, ['TRIAL219','TRIAL222_x','TRIAL222_y','TRIAL222'])\n",
    "\n",
    "# Rename\n",
    "retailer_etl = pd.merge(retailer_etl, sales_territory, on='SALES_TERRITORY_CODE')\\\n",
    "    .rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "retailer_etl = filterColumns(retailer_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(retailer_etl,22)\n",
    "retailer_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Retailer', retailer_etl, 'RETAILER_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 7 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "order_etl = pd.merge(order_header, order_method, on='ORDER_METHOD_CODE').rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude redundant foreign key columns\n",
    "# RETAILER_SITE_code can be derived from RETAILER_CONTACT_id\n",
    "# SALES_BRANCH_code can be derived from SALES_STAFF_id\n",
    "order_etl = excludeColumns(order_etl, ['RETAILER_SITE_id', 'SALES_BRANCH_id'])\n",
    "\n",
    "# Exclude\n",
    "order_etl = filterColumns(order_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(order_etl,7)\n",
    "order_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Orders', order_etl, 'ORDER_TABLE_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return reason ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Rename\n",
    "return_reason_etl = return_reason.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "return_reason_etl = filterColumns(return_reason_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(return_reason_etl,2)\n",
    "return_reason_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Return_Reason', return_reason_etl, 'RETURN_REASON_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returned Item ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 5 columns\n"
     ]
    }
   ],
   "source": [
    "# Rename \n",
    "returned_item_etl = returned_item.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude \n",
    "returned_item_etl = filterColumns(returned_item_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(returned_item_etl,5)\n",
    "returned_item_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Returns', returned_item_etl, 'RETURNS_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Details ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 7 columns\n"
     ]
    }
   ],
   "source": [
    "# Rename\n",
    "order_detail_etl = order_details.rename(columns=rename_mapping)\n",
    "\n",
    "# Exclude\n",
    "order_detail_etl = filterColumns(order_detail_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(order_detail_etl,7)\n",
    "order_detail_etl\n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Order_Details', order_detail_etl, 'ORDER_DETAIL_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Target ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 5 columns\n"
     ]
    }
   ],
   "source": [
    "# Rename\n",
    "sales_target_etl = SALES_TARGETData.rename(columns=rename_mapping)\n",
    "sales_target_etl = sales_target_etl.rename(columns={'Id':'TARGET_id'})\n",
    "\n",
    "# Exclude\n",
    "sales_target_etl = filterColumns(sales_target_etl)\n",
    "\n",
    "# Assert\n",
    "sizeCheck(sales_target_etl,5)\n",
    "sales_target_etl  \n",
    "\n",
    "# Create\n",
    "etl_tables.append(('Sales_Target', sales_target_etl, 'TARGET_id'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Product\n",
      "Inserted Product\n",
      "Creating Sales_Staff\n",
      "Inserted Sales_Staff\n",
      "Creating Satisfaction_Type\n",
      "Inserted Satisfaction_Type\n",
      "Creating Course\n",
      "Inserted Course\n",
      "Creating Sales_Forecast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted Sales_Forecast\n",
      "Creating Retailer_Contact\n",
      "Inserted Retailer_Contact\n",
      "Creating Retailer\n",
      "Inserted Retailer\n",
      "Creating Orders\n",
      "Inserted Orders\n",
      "Creating Return_Reason\n",
      "Inserted Return_Reason\n",
      "Creating Returns\n",
      "Inserted Returns\n",
      "Creating Order_Details\n",
      "Inserted Order_Details\n",
      "Creating Sales_Target\n",
      "Inserted Sales_Target\n"
     ]
    }
   ],
   "source": [
    "# Drop old\n",
    "for table in etl_tables:\n",
    "    table_name = table[0]\n",
    "    cursor.execute(f\"DROP TABLE {table_name}\")\n",
    "try:\n",
    "    cursor.commit()\n",
    "except pyodbc.Error as e:\n",
    "    print(e)\n",
    "\n",
    "# Create\n",
    "for table in etl_tables:\n",
    "    print(f\"Creating {table[0]}\")\n",
    "    createTable(table[0], table[1], table[2])\n",
    "    insertTable(table[0], table[1], table[2])\n",
    "    print(f\"Inserted {table[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr4-3-gQyTh7EJ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
